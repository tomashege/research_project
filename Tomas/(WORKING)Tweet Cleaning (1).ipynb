{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "import string\n",
    "import sklearn.feature_extraction\n",
    "import itertools\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "#DetectorFactory.seed = 0\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from normalise import normalise\n",
    "import numpy as np\n",
    "import preprocessor as p\n",
    "debug = lambda x: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_punc = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "stop_words = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_word = set(stop_words)\n",
    "new_words_temp = [\"RT\",\"new\", \"...\", \"I\", \"http\"] #<<<< ADD MORE HERE\n",
    "for i in new_words_temp:\n",
    "    temp_word.add(i)\n",
    "stop_words = frozenset(temp_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_json('/Users/tomashegewisch/Downloads/030110/LIM368_20200307.json', lines=True, orient='record')\n",
    "#tweets = pd.read_json('/Users/tomashegewisch/Downloads/030110/test.json', lines=True, orient='record')\n",
    "\n",
    "pd.set_option('display.max_rows', tweets.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "779\n",
      "\n",
      "AFTER\n",
      "779\n"
     ]
    }
   ],
   "source": [
    "debug(\"BEFORE\")\n",
    "debug(len(tweets))\n",
    "tweets.drop_duplicates(subset=['id'], keep=\"first\", inplace=True)\n",
    "debug(\"\\nAFTER\")\n",
    "debug(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'conversation_id', 'created_at', 'date', 'time', 'timezone',\n",
       "       'user_id', 'username', 'name', 'place', 'tweet', 'mentions', 'urls',\n",
       "       'photos', 'replies_count', 'retweets_count', 'likes_count', 'hashtags',\n",
       "       'cashtags', 'link', 'retweet', 'quote_url', 'video', 'near', 'geo',\n",
       "       'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to',\n",
       "       'retweet_date', 'translate', 'trans_src', 'trans_dest'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23:09:34'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['time'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_word(word):\n",
    "    if (len(word) > 1) and (word[0] == '#' or word[0] == '@'):\n",
    "        return word[0] + word[1:].translate(rm_punc).strip()\n",
    "    if word.startswith('http') or word.startswith('pic.twitter.com') or word.endswith('.com') or word.endswith('.co.za'):\n",
    "        return ''\n",
    "    if word in stop_words:\n",
    "        return ''\n",
    "    return word.translate(rm_punc).strip()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = unidecode.unidecode(tweet).lower().split()\n",
    "    tweet = [clean_tweet_word(x) for x in tweet]\n",
    "    tweet = list(itertools.chain.from_iterable([x.split() for x in tweet if x != '']))\n",
    "    tweet = [x for x in tweet if len(x) > 1]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the DF(before): 1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1117/1117 [00:15<00:00, 72.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Recognisable text: 166\n",
      "we found: 340 non-eng tweets\n",
      "The length of the DF(after): 777\n"
     ]
    }
   ],
   "source": [
    "# we want to get rid of all the tweets that do not have english in them\n",
    "# print(\"The length of the DF(before): \" + str(len(tweets)))\n",
    "# non_text = 0\n",
    "# non_en = 0\n",
    "# temp_list_non_eng = []\n",
    "# for i in tqdm(tweets['tweet'].tolist()):\n",
    "#     try:\n",
    "#         value = detect(i)\n",
    "#         if value != \"en\":\n",
    "#             temp_list_non_eng.append(i)\n",
    "#     except:\n",
    "#         non_text +=1\n",
    "# print(\"Non-Recognisable text: \" + str(non_text))        \n",
    "# print(\"we found: \"+str(len(temp_list_non_eng))+ \" non-eng tweets\")\n",
    "\n",
    "# for i in temp_list_non_eng:\n",
    "#     tweets = tweets[tweets.tweet != i]\n",
    "    \n",
    "# print(\"The length of the DF(after): \" + str(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"en\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117\n",
      "779\n"
     ]
    }
   ],
   "source": [
    "#lanagauges = tweets['tweet'].apply(lanagauge)\n",
    "#tweets['lanagauges'] = lanagauges\n",
    "#TODO:Drop the non-English words\n",
    "lanagauge(\"37Nna ke re kgane o lemogile gore team eo ya ren...\")\n",
    "print(len(tweets))\n",
    "tweets = tweets[tweets['tweet'].apply(language) == \"en\"]\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1477000386\n",
      "0794172654\n",
      "0734307920\n"
     ]
    }
   ],
   "source": [
    "# phone number\n",
    "#txt = \"The rain in Spain 0218879544\"\n",
    "#re.findall(\"\\d+\", txt)\n",
    "for i in tweets['tweet']:\n",
    "    for j in re.findall(\"\\d+\", i):\n",
    "        if len(j) == 10:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokinized = tweets['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transform = sklearn.feature_extraction.text.TfidfVectorizer(analyzer=clean_tweet, min_df=0.01)\n",
    "X = tfidf_transform.fit_transform(tweets['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#absaprem',\n",
       " '#eababanights',\n",
       " '#ultimateloveng',\n",
       " 'africa',\n",
       " 'baroka',\n",
       " 'black',\n",
       " 'campus',\n",
       " 'can',\n",
       " 'come',\n",
       " 'day',\n",
       " 'did',\n",
       " 'don',\n",
       " 'eish',\n",
       " 'fc',\n",
       " 'good',\n",
       " 'hope',\n",
       " 'it',\n",
       " 'just',\n",
       " 'kids',\n",
       " 'know',\n",
       " 'leopards',\n",
       " 'let',\n",
       " 'like',\n",
       " 'limpopo',\n",
       " 'lol',\n",
       " 'love',\n",
       " 'mall',\n",
       " 'need',\n",
       " 'north',\n",
       " 'people',\n",
       " 'polokwane',\n",
       " 're',\n",
       " 'south',\n",
       " 'team',\n",
       " 'thank',\n",
       " 'that',\n",
       " 'time',\n",
       " 'today',\n",
       " 'turfloop',\n",
       " 'win',\n",
       " 'ya',\n",
       " 'yes',\n",
       " 'you']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transform.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokinized\n",
    "tweets['tokinized'] = tokinized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#KeaDrive\n",
      "['#keadrive']\n"
     ]
    }
   ],
   "source": [
    "print(tweets['tweet'][1000])\n",
    "print(tweets['tokinized'][1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing NLTK stuff manualy\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two hundred and eighteen million, eight hundred and seventy nine thousand, five hundred and forty four',\n",
       " 'twenty seven',\n",
       " 'five hundred billion',\n",
       " 'S A B C']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"0218879544\", \"27\", \"500billion\", \"SABC\"]\n",
    "normalised_words = normalise(text, verbose=False)\n",
    "normalised_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_func(tok_words_list):\n",
    "    return normalise(tok_words_list, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised = tweets['tokinized'].apply(normalise_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['normalised'] = normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Frequency\n",
      "Word                      \n",
      "it                      31\n",
      "just                    22\n",
      "like                    21\n",
      "polokwane               21\n",
      "people                  20\n",
      "you                     17\n",
      "limpopo                 17\n",
      "don                     16\n",
      "time                    16\n",
      "know                    15\n",
      "baroka                  14\n",
      "black                   14\n",
      "team                    13\n",
      "love                    13\n",
      "leopards                13\n",
      "need                    12\n",
      "re                      12\n",
      "#ultimateloveng         12\n",
      "fc                      12\n",
      "mall                    11\n",
      "good                    11\n",
      "#absaprem               11\n",
      "eish                    11\n",
      "win                     10\n",
      "let                     10\n",
      "thank                   10\n",
      "come                    10\n",
      "hope                     9\n",
      "lol                      9\n",
      "today                    9\n",
      "did                      9\n",
      "that                     9\n",
      "#eababanights            9\n",
      "can                      9\n",
      "turfloop                 9\n",
      "north                    9\n",
      "africa                   9\n",
      "yes                      8\n",
      "game                     8\n",
      "kids                     8\n",
      "day                      8\n",
      "ve                       8\n",
      "ya                       8\n",
      "campus                   8\n",
      "south                    8\n",
      "best                     7\n",
      "league                   7\n",
      "mean                     7\n",
      "live                     7\n",
      "sorry                    7\n",
      "say                      7\n",
      "what                     7\n",
      "le                       7\n",
      "beautiful                7\n",
      "guys                     7\n",
      "university               7\n",
      "bring                    7\n",
      "mankweng                 7\n",
      "guy                      6\n",
      "tell                     6\n",
      "ok                       6\n",
      "make                     6\n",
      "way                      6\n",
      "10                       6\n",
      "man                      6\n",
      "years                    6\n",
      "brother                  6\n",
      "home                     6\n",
      "want                     6\n",
      "ke                       6\n",
      "mara                     6\n",
      "really                   6\n",
      "wena                     6\n",
      "#cafcl                   6\n",
      "thought                  6\n",
      "think                    6\n",
      "use                      6\n",
      "chiefs                   5\n",
      "coming                   5\n",
      "ll                       5\n",
      "lost                     5\n",
      "god                      5\n",
      "better                   5\n",
      "going                    5\n",
      "work                     5\n",
      "sure                     5\n",
      "doesn                    5\n",
      "wanna                    5\n",
      "check                    5\n",
      "hands                    5\n",
      "business                 5\n",
      "tv                       5\n",
      "ready                    5\n",
      "looking                  5\n",
      "me                       5\n",
      "life                     5\n",
      "#barokafc                5\n",
      "#lennakemoroka           5\n",
      "short                    5\n",
      "wash                     5\n"
     ]
    }
   ],
   "source": [
    "words = tweets['tokinized'].tolist()\n",
    "words = [item for sublist in words for item in sublist]\n",
    "\n",
    "# generate DF out of Counter\n",
    "rslt = pd.DataFrame(Counter(words).most_common(100),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is using the preprocessor libaray That we found on the intenet.\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "def preprocessing_func(tweet_text):\n",
    "    try:\n",
    "        return p.clean(str(tweet_text))\n",
    "    except:\n",
    "        print(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_output = tweets['tweet'].apply(preprocessing_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRo'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_func(\"BRo ðŸ˜‚ðŸ˜‚ 1235271459259518979\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tweet-preprocessor library to do initial preprocessing\n",
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.HASHTAG, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER) \n",
    "# Should we remove mentions and hashtags? \n",
    "# Need to keep track of people like government officials; have a column in dataframe dedicated to hashtags\n",
    "def preprocessing_func(text):\n",
    "    print(type(text)) \n",
    "    return p.clean(str(tweet['tweet']))\n",
    "        \n",
    "def clean_tweet(tweet):\n",
    "    tweet = preprocessing_func(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-b24f3d8e6e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclean_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"JEy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#p.clean(str(\"hey bro, WHat is your name... ðŸ˜‚ðŸ˜‚ 12352714\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#t = clean_tweet(\"hey bro, WHat is your name... ðŸ˜‚ðŸ˜‚ 12352714\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-8e2dd9673473>\u001b[0m in \u001b[0;36mpreprocessing_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "preprocessing_func(\"hey\")\n",
    "clean_tweet(\"JEy\")\n",
    "#p.clean(str(\"hey bro, WHat is your name... ðŸ˜‚ðŸ˜‚ 12352714\"))\n",
    "#t = clean_tweet(\"hey bro, WHat is your name... ðŸ˜‚ðŸ˜‚ 12352714\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "def run(t):\n",
    "    print( type(t) )\n",
    "def hi(test):\n",
    "    run(test)\n",
    "    \n",
    "hi(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hei'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removal of numbers <- turn this into a function\n",
    "#for i in tweets['tweet']:\n",
    "#    for j in re.findall(\"\\d+\", i):\n",
    "#        if len(j) == 10:\n",
    "#            print(j)\n",
    "            \n",
    "def remove_number_from_word(word):\n",
    "    if re.search(\"\\d+\", word) is None:\n",
    "        return word\n",
    "    else:\n",
    "        return \"\"\n",
    "        \n",
    "remove_number_from_word(\"hei\")\n",
    "#type(re.search(\"\\d+\", \"yo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
