{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomashegewisch/research_project/Tomas/env/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/tomashegewisch/research_project/Tomas/env/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unidecode\n",
    "import string\n",
    "import sklearn.feature_extraction\n",
    "import itertools\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "#DetectorFactory.seed = 0\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from normalise import normalise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_punc = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "stop_words = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_word = set(stop_words)\n",
    "temp_word.add(\"RT\") #<<< ADD MORE WORDS>>>\n",
    "temp_word.add(\"RT\")\n",
    "temp_word.add(\"The\")\n",
    "temp_word.add(\"new\")\n",
    "temp_word.add(\"-\")\n",
    "temp_word.add(\"I\")\n",
    "temp_word.add(\".\")\n",
    "temp_word.add(\"virus\")\n",
    "temp_word.add(\"http\")\n",
    "temp_word.add(\"...\")\n",
    "stop_words = frozenset(temp_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_json('/Users/tomashegewisch/Downloads/030110/LIM368_20200307.json', lines=True, orient='record')\n",
    "#tweets = pd.read_json('/Users/tomashegewisch/Downloads/030110/test.json', lines=True, orient='record')\n",
    "\n",
    "pd.set_option('display.max_rows', tweets.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "1117\n",
      "\n",
      "AFTER\n",
      "1117\n"
     ]
    }
   ],
   "source": [
    "print(\"BEFORE\")\n",
    "print(len(tweets))\n",
    "tweets.drop_duplicates(subset=['id'], keep=\"first\", inplace=True)\n",
    "print(\"\\nAFTER\")\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Since we no longer doing the same slots. I get a chance to listen to u ðŸ™‚'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tweet'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet_word(word):\n",
    "    if (len(word) > 1) and (word[0] == '#' or word[0] == '@'):\n",
    "        return word[0] + word[1:].translate(rm_punc).strip()\n",
    "    if word.startswith('http') or word.startswith('pic.twitter.com') or word.endswith('.com') or word.endswith('.co.za'):\n",
    "        return ''\n",
    "    if word in stop_words:\n",
    "        return ''\n",
    "    return word.translate(rm_punc).strip()\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = unidecode.unidecode(tweet).lower().split()\n",
    "    tweet = [clean_tweet_word(x) for x in tweet]\n",
    "    tweet = list(itertools.chain.from_iterable([x.split() for x in tweet if x != '']))\n",
    "    tweet = [x for x in tweet if len(x) > 1]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the DF(before): 1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1117/1117 [00:29<00:00, 38.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Recognisable text: 166\n",
      "we found: 339 non-eng tweets\n",
      "The length of the DF(after): 778\n"
     ]
    }
   ],
   "source": [
    "# we want to get rid of all the tweets that do not have english in them\n",
    "print(\"The length of the DF(before): \" + str(len(tweets)))\n",
    "non_text = 0\n",
    "non_en = 0\n",
    "temp_list_non_eng = []\n",
    "for i in tqdm(tweets['tweet'].tolist()):\n",
    "    try:\n",
    "        value = detect(i)\n",
    "        if value != \"en\":\n",
    "            temp_list_non_eng.append(i)\n",
    "            #print(value+\"----\" + i)\n",
    "    except:\n",
    "        non_text +=1\n",
    "print(\"Non-Recognisable text: \" + str(non_text))        \n",
    "print(\"we found: \"+str(len(temp_list_non_eng))+ \" non-eng tweets\")\n",
    "\n",
    "for i in temp_list_non_eng:\n",
    "    tweets = tweets[tweets.tweet != i]\n",
    "    \n",
    "print(\"The length of the DF(after): \" + str(len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1477000386\n",
      "0794172654\n",
      "0734307920\n"
     ]
    }
   ],
   "source": [
    "# phone number\n",
    "#txt = \"The rain in Spain 0218879544\"\n",
    "#re.findall(\"\\d+\", txt)\n",
    "for i in tweets['tweet']:\n",
    "    for j in re.findall(\"\\d+\", i):\n",
    "        if len(j) == 10:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokinized = tweets['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transform = sklearn.feature_extraction.text.TfidfVectorizer(analyzer=clean_tweet, min_df=0.01)\n",
    "X = tfidf_transform.fit_transform(tweets['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#absaprem',\n",
       " '#eababanights',\n",
       " '#ultimateloveng',\n",
       " 'africa',\n",
       " 'baroka',\n",
       " 'black',\n",
       " 'campus',\n",
       " 'can',\n",
       " 'come',\n",
       " 'day',\n",
       " 'did',\n",
       " 'don',\n",
       " 'eish',\n",
       " 'fc',\n",
       " 'good',\n",
       " 'hope',\n",
       " 'it',\n",
       " 'just',\n",
       " 'kids',\n",
       " 'know',\n",
       " 'leopards',\n",
       " 'let',\n",
       " 'like',\n",
       " 'limpopo',\n",
       " 'love',\n",
       " 'mall',\n",
       " 'need',\n",
       " 'north',\n",
       " 'people',\n",
       " 'polokwane',\n",
       " 're',\n",
       " 'say',\n",
       " 'team',\n",
       " 'thank',\n",
       " 'that',\n",
       " 'time',\n",
       " 'today',\n",
       " 'turfloop',\n",
       " 'win',\n",
       " 'ya',\n",
       " 'yes',\n",
       " 'you']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transform.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokinized\n",
    "tweets['tokinized'] = tokinized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#KeaDrive\n",
      "['#keadrive']\n"
     ]
    }
   ],
   "source": [
    "print(tweets['tweet'][1000])\n",
    "print(tweets['tokinized'][1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing NLTK stuff manualy\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"On\", \"the\", \"28\", \"Apr.\", \"2010\", \",\", \"Dr.\", \"Banks\", \"bought\", \"a\", \"chair\", \"for\", \"Â£35\", \".\"]\n",
    "# normalised_words = normalise(text, verbose=False)\n",
    "# normalised_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_func(tok_words_list):\n",
    "    #print(tok_words_list)\n",
    "    return normalise(tok_words_list, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised = tweets['tokinized'].apply(normalise_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['normalised'] = normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
