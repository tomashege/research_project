{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle(\"/Users/tomashegewisch/Desktop/all_6_months.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>...</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "      <th>tokenised</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1233905700096630789</td>\n",
       "      <td>1233905700096630784</td>\n",
       "      <td>2020-03-01 00:03:38</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>02:03:38</td>\n",
       "      <td>200</td>\n",
       "      <td>211643798</td>\n",
       "      <td>ibogard</td>\n",
       "      <td>Mike B.</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-33.91874859...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[yaaaoooo, uncle, mel, amp, anut, sandra, meet...</td>\n",
       "      <td>when your Uncle Mel &amp;amp; Anut Sandra meet you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1233956189337214976</td>\n",
       "      <td>1233956189337214976</td>\n",
       "      <td>2020-03-01 03:24:15</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>05:24:15</td>\n",
       "      <td>200</td>\n",
       "      <td>2361589272</td>\n",
       "      <td>wakandagoddess_</td>\n",
       "      <td>Uchechi ðŸ¦‹</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[imagine, just, coronavirus, infected, person,...</td>\n",
       "      <td>Imagine if just one infected person attended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233960709102456832</td>\n",
       "      <td>1233960709102456832</td>\n",
       "      <td>2020-03-01 03:42:13</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>05:42:13</td>\n",
       "      <td>200</td>\n",
       "      <td>2174223622</td>\n",
       "      <td>swazimagagula</td>\n",
       "      <td>Swati</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[ladies, lets, settle, ultimate, crush, zonke,...</td>\n",
       "      <td>Ladies let's settle this, once and for all. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1233980779723509761</td>\n",
       "      <td>1233980779723509760</td>\n",
       "      <td>2020-03-01 05:01:58</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>07:01:58</td>\n",
       "      <td>200</td>\n",
       "      <td>52679625</td>\n",
       "      <td>photosb</td>\n",
       "      <td>Steve Bailey</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-34.47161, 1...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[cardinal, woodpecker, birdphotography, bird_l...</td>\n",
       "      <td>The Cardinal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1233981539232419840</td>\n",
       "      <td>1233981539232419840</td>\n",
       "      <td>2020-03-01 05:04:59</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>07:04:59</td>\n",
       "      <td>200</td>\n",
       "      <td>1540477321</td>\n",
       "      <td>thelifesway</td>\n",
       "      <td>The Life's Way</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-26.04255582...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[time, sunday, morning, fun, samsungmobilesa, ...</td>\n",
       "      <td>Time for some Sunday morning fun with samsungm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>1235555425245958144</td>\n",
       "      <td>1235555425245958144</td>\n",
       "      <td>2020-03-05 13:19:03</td>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>15:19:03</td>\n",
       "      <td>200</td>\n",
       "      <td>4340392636</td>\n",
       "      <td>msweliteddy</td>\n",
       "      <td>Teddy Msweli</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[cancel, durban, july, ayikho, safe, coronavir...</td>\n",
       "      <td>Cancel Durban July ayikho safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1235555491591274496</td>\n",
       "      <td>1235555491591274496</td>\n",
       "      <td>2020-03-05 13:19:19</td>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>15:19:19</td>\n",
       "      <td>200</td>\n",
       "      <td>464549023</td>\n",
       "      <td>peketsi_m</td>\n",
       "      <td>keng bothata?</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[jo, nna, weeee, coronavirussa]</td>\n",
       "      <td>Jo nna weeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1235555549984382976</td>\n",
       "      <td>1235555549984382976</td>\n",
       "      <td>2020-03-05 13:19:33</td>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>15:19:33</td>\n",
       "      <td>200</td>\n",
       "      <td>135760707</td>\n",
       "      <td>b_bobby_</td>\n",
       "      <td>NaakMusiq</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[need, team, stop, corona, nonsense, coronaale...</td>\n",
       "      <td>We need our A Team to stop is corona nonsense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1235555556875632641</td>\n",
       "      <td>1235555556875632640</td>\n",
       "      <td>2020-03-05 13:19:34</td>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>15:19:34</td>\n",
       "      <td>200</td>\n",
       "      <td>1161346826701869063</td>\n",
       "      <td>misskhanyi3</td>\n",
       "      <td>Miss_khanyi</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[coronavirus, south, africa, taking, chances, ...</td>\n",
       "      <td>Coronavirus in South Africa Am not taking any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1235555760505057285</td>\n",
       "      <td>1235554561588424704</td>\n",
       "      <td>2020-03-05 13:20:23</td>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>15:20:23</td>\n",
       "      <td>200</td>\n",
       "      <td>304492141</td>\n",
       "      <td>brandonkhambule</td>\n",
       "      <td>Brandon Khambule</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[coronavirussa]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>934 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id      conversation_id          created_at       date  \\\n",
       "0    1233905700096630789  1233905700096630784 2020-03-01 00:03:38 2020-03-01   \n",
       "1    1233956189337214976  1233956189337214976 2020-03-01 03:24:15 2020-03-01   \n",
       "2    1233960709102456832  1233960709102456832 2020-03-01 03:42:13 2020-03-01   \n",
       "3    1233980779723509761  1233980779723509760 2020-03-01 05:01:58 2020-03-01   \n",
       "4    1233981539232419840  1233981539232419840 2020-03-01 05:04:59 2020-03-01   \n",
       "..                   ...                  ...                 ...        ...   \n",
       "993  1235555425245958144  1235555425245958144 2020-03-05 13:19:03 2020-03-05   \n",
       "995  1235555491591274496  1235555491591274496 2020-03-05 13:19:19 2020-03-05   \n",
       "996  1235555549984382976  1235555549984382976 2020-03-05 13:19:33 2020-03-05   \n",
       "997  1235555556875632641  1235555556875632640 2020-03-05 13:19:34 2020-03-05   \n",
       "999  1235555760505057285  1235554561588424704 2020-03-05 13:20:23 2020-03-05   \n",
       "\n",
       "         time  timezone              user_id         username  \\\n",
       "0    02:03:38       200            211643798          ibogard   \n",
       "1    05:24:15       200           2361589272  wakandagoddess_   \n",
       "2    05:42:13       200           2174223622    swazimagagula   \n",
       "3    07:01:58       200             52679625          photosb   \n",
       "4    07:04:59       200           1540477321      thelifesway   \n",
       "..        ...       ...                  ...              ...   \n",
       "993  15:19:03       200           4340392636      msweliteddy   \n",
       "995  15:19:19       200            464549023        peketsi_m   \n",
       "996  15:19:33       200            135760707         b_bobby_   \n",
       "997  15:19:34       200  1161346826701869063      misskhanyi3   \n",
       "999  15:20:23       200            304492141  brandonkhambule   \n",
       "\n",
       "                 name                                              place  ...  \\\n",
       "0             Mike B.  {'type': 'Point', 'coordinates': [-33.91874859...  ...   \n",
       "1           Uchechi ðŸ¦‹                                                     ...   \n",
       "2               Swati                                                     ...   \n",
       "3        Steve Bailey  {'type': 'Point', 'coordinates': [-34.47161, 1...  ...   \n",
       "4      The Life's Way  {'type': 'Point', 'coordinates': [-26.04255582...  ...   \n",
       "..                ...                                                ...  ...   \n",
       "993      Teddy Msweli                                                     ...   \n",
       "995     keng bothata?                                                     ...   \n",
       "996         NaakMusiq                                                     ...   \n",
       "997       Miss_khanyi                                                     ...   \n",
       "999  Brandon Khambule                                                     ...   \n",
       "\n",
       "    user_rt_id user_rt retweet_id reply_to retweet_date  translate  trans_src  \\\n",
       "0                                       []                                      \n",
       "1                                       []                                      \n",
       "2                                       []                                      \n",
       "3                                       []                                      \n",
       "4                                       []                                      \n",
       "..         ...     ...        ...      ...          ...        ...        ...   \n",
       "993                                     []                                      \n",
       "995                                     []                                      \n",
       "996                                     []                                      \n",
       "997                                     []                                      \n",
       "999                                     []                                      \n",
       "\n",
       "     trans_dest                                          tokenised  \\\n",
       "0                [yaaaoooo, uncle, mel, amp, anut, sandra, meet...   \n",
       "1                [imagine, just, coronavirus, infected, person,...   \n",
       "2                [ladies, lets, settle, ultimate, crush, zonke,...   \n",
       "3                [cardinal, woodpecker, birdphotography, bird_l...   \n",
       "4                [time, sunday, morning, fun, samsungmobilesa, ...   \n",
       "..          ...                                                ...   \n",
       "993              [cancel, durban, july, ayikho, safe, coronavir...   \n",
       "995                                [jo, nna, weeee, coronavirussa]   \n",
       "996              [need, team, stop, corona, nonsense, coronaale...   \n",
       "997              [coronavirus, south, africa, taking, chances, ...   \n",
       "999                                                [coronavirussa]   \n",
       "\n",
       "                                           tweet_clean  \n",
       "0    when your Uncle Mel &amp; Anut Sandra meet you...  \n",
       "1         Imagine if just one infected person attended  \n",
       "2    Ladies let's settle this, once and for all. Wh...  \n",
       "3                                         The Cardinal  \n",
       "4    Time for some Sunday morning fun with samsungm...  \n",
       "..                                                 ...  \n",
       "993                     Cancel Durban July ayikho safe  \n",
       "995                                       Jo nna weeee  \n",
       "996      We need our A Team to stop is corona nonsense  \n",
       "997  Coronavirus in South Africa Am not taking any ...  \n",
       "999                                                     \n",
       "\n",
       "[934 rows x 38 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [yaaaoooo, uncle, mel, amp, anut, sandra, meet...\n",
       "1      [imagine, just, coronavirus, infected, person,...\n",
       "2      [ladies, lets, settle, ultimate, crush, zonke,...\n",
       "3      [cardinal, woodpecker, birdphotography, bird_l...\n",
       "4      [time, sunday, morning, fun, samsungmobilesa, ...\n",
       "                             ...                        \n",
       "993    [cancel, durban, july, ayikho, safe, coronavir...\n",
       "995                      [jo, nna, weeee, coronavirussa]\n",
       "996    [need, team, stop, corona, nonsense, coronaale...\n",
       "997    [coronavirus, south, africa, taking, chances, ...\n",
       "999                                      [coronavirussa]\n",
       "Name: tokenised, Length: 934, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tokenised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0o', '0s', '3a', '3b', '3d', '6b', '6o', 'a', 'a1', 'a2', 'a3', 'a4', 'ab', 'able', 'about', 'above', 'abst', 'ac', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'ad', 'added', 'adj', 'ae', 'af', 'affected', 'affecting', 'affects', 'after', 'afterwards', 'ag', 'again', 'against', 'ah', 'ain', \"ain't\", 'aj', 'al', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ao', 'ap', 'apart', 'apparently', 'appear', 'appreciate', 'appropriate', 'approximately', 'ar', 'are', 'aren', 'arent', \"aren't\", 'arise', 'around', 'as', \"a's\", 'aside', 'ask', 'asking', 'associated', 'at', 'au', 'auth', 'av', 'available', 'aw', 'away', 'awfully', 'ax', 'ay', 'az', 'b', 'b1', 'b2', 'b3', 'ba', 'back', 'bc', 'bd', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'begin', 'beginning', 'beginnings', 'begins', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bi', 'bill', 'biol', 'bj', 'bk', 'bl', 'bn', 'both', 'bottom', 'bp', 'br', 'brief', 'briefly', 'bs', 'bt', 'bu', 'but', 'bx', 'by', 'c', 'c1', 'c2', 'c3', 'ca', 'call', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'cc', 'cd', 'ce', 'certain', 'certainly', 'cf', 'cg', 'ch', 'changes', 'ci', 'cit', 'cj', 'cl', 'clearly', 'cm', \"c'mon\", 'cn', 'co', 'com', 'come', 'comes', 'con', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'couldn', 'couldnt', \"couldn't\", 'course', 'cp', 'cq', 'cr', 'cry', 'cs', \"c's\", 'ct', 'cu', 'currently', 'cv', 'cx', 'cy', 'cz', 'd', 'd2', 'da', 'date', 'dc', 'dd', 'de', 'definitely', 'describe', 'described', 'despite', 'detail', 'df', 'di', 'did', 'didn', \"didn't\", 'different', 'dj', 'dk', 'dl', 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', 'done', \"don't\", 'down', 'downwards', 'dp', 'dr', 'ds', 'dt', 'du', 'due', 'during', 'dx', 'dy', 'e', 'e2', 'e3', 'ea', 'each', 'ec', 'ed', 'edu', 'ee', 'ef', 'effect', 'eg', 'ei', 'eight', 'eighty', 'either', 'ej', 'el', 'eleven', 'else', 'elsewhere', 'em', 'empty', 'en', 'end', 'ending', 'enough', 'entirely', 'eo', 'ep', 'eq', 'er', 'es', 'especially', 'est', 'et', 'et-al', 'etc', 'eu', 'ev', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'ey', 'f', 'f2', 'fa', 'far', 'fc', 'few', 'ff', 'fi', 'fifteen', 'fifth', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'fix', 'fj', 'fl', 'fn', 'fo', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'forty', 'found', 'four', 'fr', 'from', 'front', 'fs', 'ft', 'fu', 'full', 'further', 'furthermore', 'fy', 'g', 'ga', 'gave', 'ge', 'get', 'gets', 'getting', 'gi', 'give', 'given', 'gives', 'giving', 'gj', 'gl', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'gr', 'greetings', 'gs', 'gy', 'h', 'h2', 'h3', 'had', 'hadn', \"hadn't\", 'happens', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'hed', \"he'd\", \"he'll\", 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'heres', \"here's\", 'hereupon', 'hers', 'herself', 'hes', \"he's\", 'hh', 'hi', 'hid', 'him', 'himself', 'his', 'hither', 'hj', 'ho', 'home', 'hopefully', 'how', 'howbeit', 'however', \"how's\", 'hr', 'hs', 'http', 'hu', 'hundred', 'hy', 'i', 'i2', 'i3', 'i4', 'i6', 'i7', 'i8', 'ia', 'ib', 'ibid', 'ic', 'id', \"i'd\", 'ie', 'if', 'ig', 'ignored', 'ih', 'ii', 'ij', 'il', \"i'll\", 'im', \"i'm\", 'immediate', 'immediately', 'importance', 'important', 'in', 'inasmuch', 'inc', 'indeed', 'index', 'indicate', 'indicated', 'indicates', 'information', 'inner', 'insofar', 'instead', 'interest', 'into', 'invention', 'inward', 'io', 'ip', 'iq', 'ir', 'is', 'isn', \"isn't\", 'it', 'itd', \"it'd\", \"it'll\", 'its', \"it's\", 'itself', 'iv', \"i've\", 'ix', 'iy', 'iz', 'j', 'jj', 'jr', 'js', 'jt', 'ju', 'just', 'k', 'ke', 'keep', 'keeps', 'kept', 'kg', 'kj', 'km', 'know', 'known', 'knows', 'ko', 'l', 'l2', 'la', 'largely', 'last', 'lately', 'later', 'latter', 'latterly', 'lb', 'lc', 'le', 'least', 'les', 'less', 'lest', 'let', 'lets', \"let's\", 'lf', 'like', 'liked', 'likely', 'line', 'little', 'lj', 'll', 'll', 'ln', 'lo', 'look', 'looking', 'looks', 'los', 'lr', 'ls', 'lt', 'ltd', 'm', 'm2', 'ma', 'made', 'mainly', 'make', 'makes', 'many', 'may', 'maybe', 'me', 'mean', 'means', 'meantime', 'meanwhile', 'merely', 'mg', 'might', 'mightn', \"mightn't\", 'mill', 'million', 'mine', 'miss', 'ml', 'mn', 'mo', 'more', 'moreover', 'most', 'mostly', 'move', 'mr', 'mrs', 'ms', 'mt', 'mu', 'much', 'mug', 'must', 'mustn', \"mustn't\", 'my', 'myself', 'n', 'n2', 'na', 'name', 'namely', 'nay', 'nc', 'nd', 'ne', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needn', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'ng', 'ni', 'nine', 'ninety', 'nj', 'nl', 'nn', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'novel', 'now', 'nowhere', 'nr', 'ns', 'nt', 'ny', 'o', 'oa', 'ob', 'obtain', 'obtained', 'obviously', 'oc', 'od', 'of', 'off', 'often', 'og', 'oh', 'oi', 'oj', 'ok', 'okay', 'ol', 'old', 'om', 'omitted', 'on', 'once', 'one', 'ones', 'only', 'onto', 'oo', 'op', 'oq', 'or', 'ord', 'os', 'ot', 'other', 'others', 'otherwise', 'ou', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'ow', 'owing', 'own', 'ox', 'oz', 'p', 'p1', 'p2', 'p3', 'page', 'pagecount', 'pages', 'par', 'part', 'particular', 'particularly', 'pas', 'past', 'pc', 'pd', 'pe', 'per', 'perhaps', 'pf', 'ph', 'pi', 'pj', 'pk', 'pl', 'placed', 'please', 'plus', 'pm', 'pn', 'po', 'poorly', 'possible', 'possibly', 'potentially', 'pp', 'pq', 'pr', 'predominantly', 'present', 'presumably', 'previously', 'primarily', 'probably', 'promptly', 'proud', 'provides', 'ps', 'pt', 'pu', 'put', 'py', 'q', 'qj', 'qu', 'que', 'quickly', 'quite', 'qv', 'r', 'r2', 'ra', 'ran', 'rather', 'rc', 'rd', 're', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related', 'relatively', 'research', 'research-articl', 'respectively', 'resulted', 'resulting', 'results', 'rf', 'rh', 'ri', 'right', 'rj', 'rl', 'rm', 'rn', 'ro', 'rq', 'rr', 'rs', 'rt', 'ru', 'run', 'rv', 'ry', 's', 's2', 'sa', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sc', 'sd', 'se', 'sec', 'second', 'secondly', 'section', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'sf', 'shall', 'shan', \"shan't\", 'she', 'shed', \"she'd\", \"she'll\", 'shes', \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'show', 'showed', 'shown', 'showns', 'shows', 'si', 'side', 'significant', 'significantly', 'similar', 'similarly', 'since', 'sincere', 'six', 'sixty', 'sj', 'sl', 'slightly', 'sm', 'sn', 'so', 'some', 'somebody', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'sp', 'specifically', 'specified', 'specify', 'specifying', 'sq', 'sr', 'ss', 'st', 'still', 'stop', 'strongly', 'sub', 'substantially', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure', 'sy', 'system', 'sz', 't', 't1', 't2', 't3', 'take', 'taken', 'taking', 'tb', 'tc', 'td', 'te', 'tell', 'ten', 'tends', 'tf', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", \"that've\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'thered', 'therefore', 'therein', \"there'll\", 'thereof', 'therere', 'theres', \"there's\", 'thereto', 'thereupon', \"there've\", 'these', 'they', 'theyd', \"they'd\", \"they'll\", 'theyre', \"they're\", \"they've\", 'thickv', 'thin', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'thou', 'though', 'thoughh', 'thousand', 'three', 'throug', 'through', 'throughout', 'thru', 'thus', 'ti', 'til', 'tip', 'tj', 'tl', 'tm', 'tn', 'to', 'together', 'too', 'took', 'top', 'toward', 'towards', 'tp', 'tq', 'tr', 'tried', 'tries', 'truly', 'try', 'trying', 'ts', \"t's\", 'tt', 'tv', 'twelve', 'twenty', 'twice', 'two', 'tx', 'u', 'u201d', 'ue', 'ui', 'uj', 'uk', 'um', 'un', 'under', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'uo', 'up', 'upon', 'ups', 'ur', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using', 'usually', 'ut', 'v', 'va', 'value', 'various', 'vd', 've', 've', 'very', 'via', 'viz', 'vj', 'vo', 'vol', 'vols', 'volumtype', 'vq', 'vs', 'vt', 'vu', 'w', 'wa', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', 'wed', \"we'd\", 'welcome', 'well', \"we'll\", 'well-b', 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what'll\", 'whats', \"what's\", 'when', 'whence', 'whenever', \"when's\", 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'whim', 'whither', 'who', 'whod', 'whoever', 'whole', \"who'll\", 'whom', 'whomever', 'whos', \"who's\", 'whose', 'why', \"why's\", 'wi', 'widely', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wo', 'won', 'wonder', 'wont', \"won't\", 'words', 'world', 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'www', 'x', 'x1', 'x2', 'x3', 'xf', 'xi', 'xj', 'xk', 'xl', 'xn', 'xo', 'xs', 'xt', 'xv', 'xx', 'y', 'y2', 'yes', 'yet', 'yj', 'yl', 'you', 'youd', \"you'd\", \"you'll\", 'your', 'youre', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yr', 'ys', 'yt', 'z', 'zero', 'zi', 'zz']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "new_words_temp = []\n",
    "with open('/Users/tomashegewisch/research_project/Tomas/data/add_stopwords.json') as f:\n",
    "    new_words_temp = json.load(f)['stopwords']\n",
    "print(new_words_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get sentimnet for 6 months of a particular \"hashtag\"\n",
    "# def get_if_hastag(htags_list):\n",
    "#     for i in htags_list:\n",
    "#         if i == find_word:\n",
    "#             return True\n",
    "#     return False\n",
    "# def drill_down_date_hastage(topic_word):\n",
    "#     key_word = topic_word\n",
    "#     key_word_sentimnet_score_all = tweets[tweets['hashtags'].apply(lambda x : get_key_word(x,key_word)) == True]\n",
    "#     positive_date_mean = key_word_sentimnet_score_all.groupby('date', as_index=False)['positve'].mean()\n",
    "#     negitive_date_mean = key_word_sentimnet_score_all.groupby('date', as_index=False)['negitive'].mean()\n",
    "#     # ASK RICH ABOUT THIS SECTION OF CODE>>> What it does is adds the prvious value to the NaN sections.\n",
    "#     NaN_test = pd.isnull(positive_date_mean)\n",
    "#     temp = 0.0 \n",
    "#     for i in range(0, len(NaN_test)):\n",
    "#         try:\n",
    "#             if NaN_test['positve'][i] == True:\n",
    "#                 positive_date_mean['positve'][i] = temp\n",
    "#             else:\n",
    "#                 temp = positive_date_mean['positve'][i]\n",
    "#         except:\n",
    "#             continue\n",
    "#     NaN_test = pd.isnull(negitive_date_mean)\n",
    "#     temp = 0.0\n",
    "#     for i in range(0, len(NaN_test)):\n",
    "#         try:\n",
    "#             if NaN_test['negitive'][i] == True:\n",
    "#                 negitive_date_mean['negitive'][i] = temp\n",
    "#             else:\n",
    "#                 temp = negitive_date_mean['negitive'][i]\n",
    "#         except:\n",
    "#             continue \n",
    "\n",
    "#         # Dsiplay The files.\n",
    "#     display_df_average = positive_date_mean\n",
    "#     display_df_average['negitive'] = negitive_date_mean['negitive']\n",
    "#     plt.figure(figsize=(16, 6))\n",
    "#     sns.lineplot(x='Date',\n",
    "#                  y='Polarity',\n",
    "#                  hue='Key',\n",
    "#                  data=pd.melt(display_df_average, \n",
    "#                               ['date']).rename(columns={\"value\": \"Polarity\",\"variable\":\"Key\", \"date\":\"Date\"})).set_title(\"The Sentiment for of Topic: \"+key_word).get_figure().autofmt_xdate()\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"/Users/tomashegewisch/research_project/Tomas/charts/hastag_sentiment/hastag_Sentiment_\"+key_word+\"_AllMonths_.png\")\n",
    "#     plt.show()\n",
    "#     plt.clf()\n",
    "\n",
    "# list_of_hastages_to_process = ['covid19', \n",
    "# 'southafrica', \n",
    "# 'lockdownsa', \n",
    "# 'cyrilramaphosa', \n",
    "# 'lockdown', \n",
    "# 'covid19sa',\n",
    "# 'coronavirussa',\n",
    "# 'covid_19', \n",
    "# 'lockdownsouthafrica',\n",
    "# 'government']\n",
    "\n",
    "# # list_of_hastages_to_process = ['southafrica']\n",
    "# for i in list_of_hastages_to_process:\n",
    "#     drill_down_date_hastage(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_key_word(text, kw):\n",
    "#     if kw in text:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return \"no\"\n",
    "    \n",
    "# # want to find the date range \n",
    "# def get_the_date(month_code):\n",
    "#     moths_dates = {1:['2020-03-01','2020-03-31',\"March\"],\n",
    "#                   2:['2020-04-01','2020-04-30', \"April\"],\n",
    "#                   3:['2020-05-01','2020-05-31', \"May\"],\n",
    "#                   4:['2020-06-01','2020-06-30', \"June\"],\n",
    "#                   5:['2020-07-01','2020-07-31', \"July\"],\n",
    "#                   6:['2020-08-01','2020-08-31', \"August\"],\n",
    "#                   7:['2020-09-01','2020-09-30', \"September\"],\n",
    "#                   8:['2020-10-01','2020-10-31', \"October\"]}\n",
    "#     return moths_dates[month_code]\n",
    "\n",
    "# # retrunr if the date is not in the rage\n",
    "# def get_date_range(date, d1, d2):\n",
    "#     date_1 = datetime.datetime.strptime(d1,'%Y-%m-%d')\n",
    "#     date_2 = datetime.datetime.strptime(d2,'%Y-%m-%d')\n",
    "#     date = date.to_pydatetime().strftime(\"%Y-%m-%d\")\n",
    "#     date = datetime.datetime.strptime(date,'%Y-%m-%d')\n",
    "#     if date >= date_1 and date <= date_2:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "    \n",
    "# # group by date\n",
    "\n",
    "# for i in range(1,9):\n",
    "#     nedded_dates = get_the_date(i)\n",
    "#     for i in word_list_counter.most_common(1):\n",
    "#         # select start and end date\n",
    "#         # Drop The columes that do not include the key_word\n",
    "#         key_word = i[0]\n",
    "#         key_word_sentimnet_score_all = tweets[tweets['tweet'].apply(lambda x : get_key_word(x,key_word)) == True]\n",
    "#         key_word_sentimnet_score_all = key_word_sentimnet_score_all[key_word_sentimnet_score_all['date'].apply(lambda x : get_date_range(x, nedded_dates[0], nedded_dates[1])) == True]\n",
    "#         #Group by\n",
    "#         positive_date_mean = key_word_sentimnet_score_all.groupby('date', as_index=False)['positve'].mean()\n",
    "#         negitive_date_mean = key_word_sentimnet_score_all.groupby('date', as_index=False)['negitive'].mean()\n",
    "#         # ASK RICH ABOUT THIS SECTION OF CODE>>> What it does is adds the prvious value to the NaN sections.\n",
    "#         NaN_test = pd.isnull(positive_date_mean)\n",
    "#         temp = 0.0 \n",
    "#         for i in range(0, len(NaN_test)):\n",
    "#             try:\n",
    "#                 if NaN_test['positve'][i] == True:\n",
    "\n",
    "#                     positive_date_mean['positve'][i] = temp\n",
    "#                 else:\n",
    "#                     temp = positive_date_mean['positve'][i]\n",
    "#             except:\n",
    "#                 continue\n",
    "#         NaN_test = pd.isnull(negitive_date_mean)\n",
    "#         temp = 0.0\n",
    "#         for i in range(0, len(NaN_test)):\n",
    "#             try:\n",
    "#                 if NaN_test['negitive'][i] == True:\n",
    "#                     negitive_date_mean['negitive'][i] = temp\n",
    "#                 else:\n",
    "#                     temp = negitive_date_mean['negitive'][i]\n",
    "#             except:\n",
    "#                 continue \n",
    "\n",
    "#         # Dsiplay The plot.\n",
    "#         display_df_average = positive_date_mean\n",
    "#         display_df_average['negitive'] = negitive_date_mean['negitive']\n",
    "#         plt.figure(figsize=(16, 6))\n",
    "#         sns.lineplot(x='Date', y='Polarity', hue='Key', \n",
    "#              data=pd.melt(display_df_average, ['date']).rename(columns={\"value\": \"Polarity\",\"variable\":\"Key\", \"date\":\"Date\"})).set_title(\"The Sentiment for \" + str(nedded_dates[2]) + \" of Topic: \"+key_word).get_figure().autofmt_xdate()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(\"/Users/tomashegewisch/research_project/Tomas/charts/line_sentiment/Sentiment_\"+key_word+\"_\"+str(nedded_dates[2])+\".png\")\n",
    "#         plt.show()\n",
    "#         plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the timeline frequency for popular hastags for the 6 months. \n",
    "# #ignoreing the zero dates\n",
    "# def get_time_line_of_hastages(find_word):\n",
    "#     def get_if_hastag(htags_list):\n",
    "#         for i in htags_list:\n",
    "#             if i == find_word:\n",
    "#                 return True\n",
    "#         return False\n",
    "#     specific_hastage_tweets = tweets[tweets['hashtags'].apply(lambda x : get_if_hastag(x)) == True]\n",
    "#     post_feq = specific_hastage_tweets.groupby('date', as_index=False).size()\n",
    "#     #post_feq = post_feq.head(10)\n",
    "#     plt.figure(figsize=(16, 6))\n",
    "#     sns.lineplot(x='date',\n",
    "#                  y='size',\n",
    "#                  data=post_feq).set_title(\"Frequency for: \"+str(find_word)).get_figure().autofmt_xdate()\n",
    "#     plt.tight_layout()\n",
    "#     print(\"/Users/tomashegewisch/research_project/Tomas/charts/Frequency/feq_\"+str(find_word)+\".png\")\n",
    "#     plt.savefig(\"/Users/tomashegewisch/research_project/Tomas/charts/Frequency/feq_\"+str(find_word)+\".png\")\n",
    "#     plt.show()\n",
    "#     plt.clf()\n",
    "    \n",
    "# list_of_hastages_to_process = ['covid19', \n",
    "# 'southafrica', \n",
    "# 'lockdownsa', \n",
    "# 'cyrilramaphosa', \n",
    "# 'lockdown', \n",
    "# 'covid19sa',\n",
    "# 'coronavirussa',\n",
    "# 'covid_19', \n",
    "# 'lockdownsouthafrica',\n",
    "# 'government', \n",
    "# 'cigaretteban', \n",
    "# 'alcoholban']\n",
    "\n",
    "# #list_of_hastages_to_process = ['lockdown']\n",
    "\n",
    "# for i in list_of_hastages_to_process:\n",
    "#     get_time_line_of_hastages(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drill down code\n",
    "def drill_down_date_topic(topic_word, date_1, date_2):\n",
    "    key_word = topic_word\n",
    "    nedded_dates = [date_1, date_2]\n",
    "    key_word_sentimnet_score_all = tweets[tweets['tweet'].apply(lambda x : get_key_word(x,key_word)) == True]\n",
    "    key_word_sentimnet_score_all = key_word_sentimnet_score_all[key_word_sentimnet_score_all['date'].apply(lambda x : get_date_range(x, nedded_dates[0], nedded_dates[1])) == True]\n",
    "    positive_date_mean = key_word_sentimnet_score_all.groupby('date', as_index=False)['positve'].mean()\n",
    "    negitive_date_mean = key_word_sentimnet_score_all.groupby('date', as_index=False)['negitive'].mean()\n",
    "    # ASK RICH ABOUT THIS SECTION OF CODE>>> What it does is adds the prvious value to the NaN sections.\n",
    "    NaN_test = pd.isnull(positive_date_mean)\n",
    "    temp = 0.0 \n",
    "    for i in range(0, len(NaN_test)):\n",
    "        try:\n",
    "            if NaN_test['positve'][i] == True:\n",
    "                positive_date_mean['positve'][i] = temp\n",
    "            else:\n",
    "                temp = positive_date_mean['positve'][i]\n",
    "        except:\n",
    "            continue\n",
    "    NaN_test = pd.isnull(negitive_date_mean)\n",
    "    temp = 0.0\n",
    "    for i in range(0, len(NaN_test)):\n",
    "        try:\n",
    "            if NaN_test['negitive'][i] == True:\n",
    "                negitive_date_mean['negitive'][i] = temp\n",
    "            else:\n",
    "                temp = negitive_date_mean['negitive'][i]\n",
    "        except:\n",
    "            continue \n",
    "\n",
    "        # Dsiplay The files.\n",
    "    display_df_average = positive_date_mean\n",
    "    display_df_average['negitive'] = negitive_date_mean['negitive']\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.lineplot(x='Date',\n",
    "                 y='Polarity',\n",
    "                 hue='Key',\n",
    "                 data=pd.melt(display_df_average, \n",
    "                              ['date']).rename(columns={\"value\": \"Polarity\",\"variable\":\"Key\", \"date\":\"Date\"})).set_title(\"The Sentiment for \" + str(nedded_dates) + \" of Topic: \"+key_word).get_figure().autofmt_xdate()\n",
    "    plt.tight_layout()\n",
    "    print(nedded_dates)\n",
    "    plt.show()\n",
    "    #plt.savefig(\"/Users/tomashegewisch/research_project/Tomas/charts/Sentiment_\"+key_word+\"_\"+str(nedded_dates[0])+\"_\"+str(nedded_dates[1])+\".png\")\n",
    "    plt.clf()\n",
    "\n",
    "#drill_down_date_topic(\"covid\", '2020-03-01', '2020-08-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split positive and negitive tweets into two diffrent data frame columes.\n",
    "def get_value_positive(x):\n",
    "    if x > 0.0:\n",
    "        return x\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_value_negitive(x):\n",
    "    if x < 0.0:\n",
    "        return x\n",
    "    else:\n",
    "        return None\n",
    "# we have removed the 0 from both.  \n",
    "tweets[\"positve\"] = tweets['senti_values'].apply(get_value_positive)\n",
    "tweets[\"negitive\"] = tweets['senti_values'].apply(get_value_negitive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
