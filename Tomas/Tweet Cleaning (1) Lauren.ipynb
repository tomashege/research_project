{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import re\n",
    "import sklearn.feature_extraction\n",
    "import string\n",
    "import unidecode\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "\n",
    "# Lambda function for printing\n",
    "debug = lambda x: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create punctuation removal variable\n",
    "rm_punc = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Create stopword removal variable\n",
    "stopwords = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS\n",
    "temp_word = set(stopwords)\n",
    "new_words_temp = [] # <- Add any additional stopwords here\n",
    "for i in new_words_temp:\n",
    "    temp_word.add(i)\n",
    "stopwords = frozenset(temp_word)\n",
    "\n",
    "# Create stemmer variables\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON files containing tweet dataset(s)\n",
    "tweets = pd.read_json('/Users/tomashegewisch/Downloads/030110/JHB_20200305.json', lines=True, orient='record')\n",
    "\n",
    "# View all rows contained in the dataset(s)\n",
    "pd.set_option('display.max_rows', tweets.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "1117\n",
      "\n",
      "AFTER\n",
      "1117\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate tweets\n",
    "debug(\"BEFORE\")\n",
    "debug(len(tweets))\n",
    "tweets.drop_duplicates(subset=['id'], keep=\"first\", inplace=True)\n",
    "debug(\"\\nAFTER\")\n",
    "debug(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117\n",
      "773\n"
     ]
    }
   ],
   "source": [
    "# Remove all tweets which are not English\n",
    "def language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"en\"\n",
    "    \n",
    "debug(len(tweets))\n",
    "tweets = tweets[tweets['tweet'].apply(language) == \"en\"]\n",
    "debug(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword and link removal\n",
    "def remove_stopwords(word):\n",
    "    if word in stopwords:\n",
    "        return ''\n",
    "    if word.startswith('http') or word.startswith('pictwittercom') or word.endswith('com') or word.endswith('coza'):\n",
    "        return ''\n",
    "    return word\n",
    "\n",
    "# Preprocessing the tweet\n",
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in text:\n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punctuation\n",
    "        new_text = re.sub(r'\\d+','',new_text) # remove numbers\n",
    "        new_text = re.sub('\\n', ' ', new_text) #remove escape characters\n",
    "        new_text = new_text.lower() # lower case         \n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "        temp_string = ''\n",
    "        for i in clean_data:\n",
    "            temp_string += i\n",
    "    clean_data = temp_string\n",
    "    return clean_data\n",
    "\n",
    "# Cleaning and tokenising the tweet\n",
    "def clean_tweet(tweet):\n",
    "    tweet = preprocess(tweet)\n",
    "    tweet = unidecode.unidecode(tweet).lower().split()\n",
    "    tweet = [remove_stopwords(x) for x in tweet]\n",
    "    tweet = list(itertools.chain.from_iterable([x.split() for x in tweet if x != '']))\n",
    "    tweet = [x for x in tweet if len(x) > 1]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tokenised'] = tweets['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi my name is jack the quack snort snort\n"
     ]
    }
   ],
   "source": [
    "debug(preprocess(\"Hi my name is Jack the Quack *snort snort*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_text(text):\n",
    "    if text == []:\n",
    "        return \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "904\n",
      "After\n",
      "904\n"
     ]
    }
   ],
   "source": [
    "#DROP rows that do not have text in them...\n",
    "debug(\"Before\")\n",
    "debug(len(tweets))\n",
    "tweets = tweets[tweets['tokenised'].apply(find_non_text) != \"NA\"]\n",
    "debug(\"After\")\n",
    "debug(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " lol\n",
      " thank\n",
      " don\n",
      " just\n",
      " people\n",
      " know\n",
      " like\n",
      " love\n",
      " time\n",
      " good\n",
      "Cluster 1:\n",
      " status\n",
      " https\n",
      " twitter\n",
      " com\n",
      " pic\n",
      " poladi_\n",
      " 1235553337379917824\n",
      " news24\n",
      " don\n",
      " advobarryroux\n",
      "Cluster 2:\n",
      " wish\n",
      " birthday\n",
      " happy\n",
      " best\n",
      " things\n",
      " luck\n",
      " good\n",
      " right\n",
      " day\n",
      " pleasure\n",
      "Cluster 3:\n",
      " pic\n",
      " twitter\n",
      " com\n",
      " coronavirussa\n",
      " coronaviruschallenge\n",
      " corona\n",
      " just\n",
      " coronvirus\n",
      " coronavirus\n",
      " like\n",
      "\n",
      "\n",
      "Prediction\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# clustering stuff\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X = vectorizer.fit_transform(documents)\n",
    "\n",
    "X = vectorizer.fit_transform(tweets['tweet'])\n",
    "\n",
    "true_k = 4\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Frequency\n",
      "Word                     \n",
      "im                     46\n",
      "polokwane              22\n",
      "just                   22\n",
      "like                   21\n",
      "people                 18\n",
      "dont                   17\n",
      "time                   16\n",
      "limpopo                16\n",
      "know                   15\n",
      "love                   13\n",
      "black                  13\n",
      "need                   12\n",
      "ultimateloveng         12\n",
      "baroka                 12\n",
      "leopards               12\n",
      "mall                   11\n",
      "team                   11\n",
      "good                   11\n",
      "eish                   11\n",
      "thank                  10\n",
      "come                   10\n",
      "hope                    9\n",
      "did                     9\n",
      "eababanights            9\n",
      "youre                   9\n",
      "absaprem                9\n",
      "kids                    9\n",
      "day                     9\n",
      "fc                      9\n",
      "turfloop                9\n",
      "north                   9\n",
      "africa                  9\n",
      "yes                     8\n",
      "win                     8\n",
      "game                    8\n",
      "today                   8\n",
      "lets                    8\n",
      "campus                  8\n",
      "south                   8\n",
      "best                    7\n",
      "lol                     7\n",
      "mean                    7\n",
      "ke                      7\n",
      "live                    7\n",
      "new                     7\n",
      "sorry                   7\n",
      "say                     7\n",
      "le                      7\n",
      "beautiful               7\n",
      "guys                    7\n",
      "university              7\n",
      "bring                   7\n",
      "mankweng                7\n",
      "league                  6\n",
      "guy                     6\n",
      "tell                    6\n",
      "make                    6\n",
      "way                     6\n",
      "man                     6\n",
      "ya                      6\n",
      "years                   6\n",
      "home                    6\n",
      "want                    6\n",
      "mara                    6\n",
      "really                  6\n",
      "whats                   6\n",
      "wena                    6\n",
      "cafcl                   6\n",
      "thought                 6\n",
      "think                   6\n",
      "use                     6\n",
      "chiefs                  5\n",
      "coming                  5\n",
      "lost                    5\n",
      "god                     5\n",
      "better                  5\n",
      "going                   5\n",
      "work                    5\n",
      "sure                    5\n",
      "ok                      5\n",
      "doesnt                  5\n",
      "wanna                   5\n",
      "check                   5\n",
      "teams                   5\n",
      "hands                   5\n",
      "brother                 5\n",
      "business                5\n",
      "tv                      5\n",
      "ready                   5\n",
      "looking                 5\n",
      "life                    5\n",
      "barokafc                5\n",
      "lennakemoroka           5\n",
      "short                   5\n",
      "great                   5\n",
      "used                    5\n",
      "unit                    5\n",
      "chance                  4\n",
      "definitely              4\n",
      "place                   4\n"
     ]
    }
   ],
   "source": [
    "words = tweets['tokenised'].tolist()\n",
    "words = [item for sublist in words for item in sublist]\n",
    "\n",
    "# generate DF out of Counter\n",
    "rslt = pd.DataFrame(Counter(words).most_common(100),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "debug(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
